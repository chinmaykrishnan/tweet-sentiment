{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### *15CSE350 INFORMATION RETRIEVAL*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Analysing the Feelings of Twitterati "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Setting Up Twitter Developer Credentials\n",
    "For obtaining the required credentials, you need to sign up for a twitter developer's account. You can sign up for one [here](https://developer.twitter.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall store our Twitter credentials in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# create a dictionary to store your twitter credentials\n",
    "twitter_cred = dict()\n",
    "\n",
    "# Entering the consumer_key, consumer_secret, access_key and access_secret\n",
    "twitter_cred['CONSUMER_KEY'] = 'YOUR_TWITTER_CONSUMER_KEY'\n",
    "twitter_cred['CONSUMER_SECRET'] = 'YOUR_CONSUMER_SECRET_KEY'\n",
    "twitter_cred['ACCESS_KEY'] = 'YOUR_ACCESS_KEY'\n",
    "twitter_cred['ACCESS_SECRET'] = 'YOUR_ACCESS_SECRET_KEY'\n",
    "\n",
    "# Save the information to a json so that it can be reused in code without exposing\n",
    "# the secret info to public\n",
    "\n",
    "with open('twitter_credentials.json', 'w') as secret_info:\n",
    "    json.dump(twitter_cred, secret_info, indent=4, sort_keys = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Setting Up the Twitter API Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "# Twitter API credentials\n",
    "\n",
    "with open('twitter_credentials.json') as cred_data:\n",
    "    info = json.load(cred_data)\n",
    "\n",
    "consumer_key = info['CONSUMER_KEY']\n",
    "consumer_secret = info['CONSUMER_SECRET']\n",
    "access_key = info['ACCESS_KEY']\n",
    "access_secret = info['ACCESS_SECRET']\n",
    "\n",
    "# Create the api endpoint\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Fetching and Cleaning the Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the maximum number of tweets that you want to extract- 10\n",
      "Enter the hashtag you want to scrape- isro\n",
      "Extracted 10 tweets tagged with #isro\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Mention the maximum number of tweets that you want to be extracted.\n",
    "max_tweets = int(input('Enter the maximum number of tweets that you want to extract- '))\n",
    "\n",
    "# Mention the hashtag that you want to look out for\n",
    "hashtag = input('Enter the hashtag you want to scrape- ')\n",
    "\n",
    "#Fetching tweets using our API\n",
    "\n",
    "tweets = api.search(q = \"#\"+hashtag, lang=\"en\", count = max_tweets, tweet_mode = \"extended\")\n",
    "\n",
    "clean_tweets = []\n",
    "tweet_text = []\n",
    "\n",
    "for tweet in tweets:    \n",
    "    t = tweet.full_text\n",
    "    tt = t.lower()\n",
    "    #print(\"\\n\\n'tweet in lowercase text: \", tt)\n",
    "    tweet_text.append(tt)\n",
    "   \n",
    "    #Removing urls(http:..)\n",
    "    clean_twt = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', tt)\n",
    "\n",
    "    #Removing any other punctuation or unwanted character\n",
    "    clean_twt = re.sub(r\"[.,;:$&*|?'-]\",'',clean_twt)\n",
    "    \n",
    "    #Removing mentions\n",
    "    clean_twt = re.sub('@[^\\s]+','',clean_twt)\n",
    "    \n",
    "    #print(\"\\nCleaned tweet is \", clean_twt)\n",
    "    \n",
    "    #adding to clean_tweets\n",
    "    clean_tweets.append(clean_twt)\n",
    "\n",
    "print ('Extracted ' + str(len(tweets))+ ' tweets tagged with #' + hashtag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "#Utility function that uses the textBlob library to classify the sentiment using the sentiment method\n",
    "def get_tweet_sentiment(tweet): \n",
    "    # create TextBlob object of passed tweet text \n",
    "    analysis = TextBlob(tweet) \n",
    "    \n",
    "    # set sentiment \n",
    "    if analysis.sentiment.polarity > 0: \n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity == 0: \n",
    "        return 'neutral'\n",
    "    else: \n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet is:  rt        \n",
      "just giving information to isro no need to spend money to…  sentiment is:  neutral \n",
      "\n",
      "tweet is:  #netra  indias new space warning system will also keep an eye out for hostile missiles\n",
      "\n",
      "  pabsgill\n",
      "\n",
      "isro #isro   sentiment is:  positive \n",
      "\n",
      "tweet is:  this one from table calendar bought 2 yrs back so apt after #chandrayan2 success #isro #vikramlander   sentiment is:  positive \n",
      "\n",
      "tweet is:  rt  #isro we are proud of you thanks for all the efforts you brought the whole nation together helped us pray together…  sentiment is:  positive \n",
      "\n",
      "tweet is:  this is called \"full paisa vasool\" project \n",
      "congratulations team \n",
      "planned for 6 months india’s mars mission #mangalyaan completes 5 years only artificial satellite that could image the full disc of mars in one view frame \n",
      "#isro\n",
      "#missionmangal\n",
      "\n",
      "  sentiment is:  positive \n",
      "\n",
      "tweet is:  softland failure nothing to worry about exisro chief  deccan herald \n",
      "  #nair #isro #chandrayaans   sentiment is:  negative \n",
      "\n",
      "tweet is:  rt  sometimes we don’t land or arrive at the destination we want to the important thing is we took off and had the hope and belief…  sentiment is:  positive \n",
      "\n",
      "tweet is:  congrats to  for #navic navigation system being approved by global standard #isro #isrohaitomumkinhai  sentiment is:  neutral \n",
      "\n",
      "tweet is:  rt  ▪️if you fail never give up because fail means first attempt in learning ♥️\n",
      "▪️end is not end if fact end means effort neve…  sentiment is:  negative \n",
      "\n",
      "tweet is:  rt    #isro ka #chandrayaan2 se contact hona possible he par #icicicustomercare se contact hona #asambhav…  sentiment is:  neutral \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We shall display the sentiment of each of the retrieved tweets\n",
    "for tweet in clean_tweets:\n",
    "    print(\"tweet is: \", tweet, \" sentiment is: \",get_tweet_sentiment(tweet), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Simple demo of the textblob sentiment analysers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "#Pattern Analyser - default analyser\n",
    "print(\"PatternAnalyzer() results:\")\n",
    "#Positive \n",
    "blob = TextBlob(\"This is a great place\\n\")\n",
    "print(blob.sentiment.polarity)\n",
    "\n",
    "#Neutral\n",
    "blob = TextBlob(\"This is a place\\n\")\n",
    "print(blob.sentiment.polarity)\n",
    "\n",
    "#Negative\n",
    "blob = TextBlob(\"This is a terrible place\\n\")\n",
    "print(blob.sentiment.polarity)\n",
    "\n",
    "print(\"\\nNaiveBayesAnalyzer() results:\")\n",
    "#Naive Bayes Analyser - we need to explicitly state that the NaiveBayesAnalyser() is to be used\n",
    "#Positive example\n",
    "blob = TextBlob(\"This is a great place\", analyzer = NaiveBayesAnalyzer())\n",
    "print(blob.sentiment.classification)\n",
    "\n",
    "#Negative example\n",
    "blob = TextBlob(\"This is a horrible place\", analyzer = NaiveBayesAnalyzer())\n",
    "print(blob.sentiment.classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
